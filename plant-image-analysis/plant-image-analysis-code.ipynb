{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using image classification to label plant species\n",
    "Firstly, importing all necessary packages needed and setting some plot style preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (21.0, 13.0)\n",
    "plt.rcParams['font.size'] = 18.0\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I have access to a GPU on my PC I will quickly check if TensorFlow is able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That all checks out. Now we can pre-process the multi-labeled image data downloaded from the [Kaggle competition page](https://www.kaggle.com/c/plant-seedlings-classification/overview). Firstly, we need to check how many different plant labels we have to begin with, as this will define the set of all classification labels that can possibly be assigned by our model. The image data available under $\\texttt{train.zip}$ has the labels of the plant species as the name of the subdirectory containing the images of that plant species, so we can list all subdirectories of the data to see all classification labels and save as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black-grass',\n",
       " 'Charlock',\n",
       " 'Cleavers',\n",
       " 'Common Chickweed',\n",
       " 'Common wheat',\n",
       " 'Fat Hen',\n",
       " 'Loose Silky-bent',\n",
       " 'Maize',\n",
       " 'Scentless Mayweed',\n",
       " 'Shepherds Purse',\n",
       " 'Small-flowered Cranesbill',\n",
       " 'Sugar beet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_list = os.listdir('./train')\n",
    "species_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can consider constructing a coherent pre-pixel data set which contains the plant species label as the first column, the numerical plant species label as the second column and the image file location as the third column. This is done for ease of further use of the data in that we won't need to deal with constantly going into each of the above subdirectories of the data. After some research into how this can be done, I found the following to be the most straight forward method to iteratively go through each file in each subdirectory while keeping track of the subdirectory label. Note that we also setup the resized image file name as this will be created in the next couple of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>numerical_species</th>\n",
       "      <th>file_name</th>\n",
       "      <th>pre_processed_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0050f38b3.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0050f38b3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0183fdf68.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0183fdf68.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0260cffa8.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0260cffa8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/05eedce4d.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/05eedce4d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/075d004bc.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/075d004bc.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       species  numerical_species                          file_name  \\\n",
       "0  Black-grass                  0  ./train/Black-grass/0050f38b3.png   \n",
       "1  Black-grass                  0  ./train/Black-grass/0183fdf68.png   \n",
       "2  Black-grass                  0  ./train/Black-grass/0260cffa8.png   \n",
       "3  Black-grass                  0  ./train/Black-grass/05eedce4d.png   \n",
       "4  Black-grass                  0  ./train/Black-grass/075d004bc.png   \n",
       "\n",
       "                           pre_processed_file_name  \n",
       "0  ./pre_processed_train/Black-grass/0050f38b3.png  \n",
       "1  ./pre_processed_train/Black-grass/0183fdf68.png  \n",
       "2  ./pre_processed_train/Black-grass/0260cffa8.png  \n",
       "3  ./pre_processed_train/Black-grass/05eedce4d.png  \n",
       "4  ./pre_processed_train/Black-grass/075d004bc.png  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = list()\n",
    "for numerical_species, species in enumerate(species_list):\n",
    "    for file_name in os.listdir(os.path.join('./train', species)):\n",
    "        full_data.append([species, numerical_species, './train/{}/{}'.format(species, file_name), './pre_processed_train/{}/{}'.format(species, file_name)])\n",
    "full_data = pd.DataFrame(full_data, columns=['species', 'numerical_species', 'file_name', 'pre_processed_file_name'])\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the total image count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the total number of images in each of the given species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loose Silky-bent             654\n",
       "Common Chickweed             611\n",
       "Scentless Mayweed            516\n",
       "Small-flowered Cranesbill    496\n",
       "Fat Hen                      475\n",
       "Charlock                     390\n",
       "Sugar beet                   385\n",
       "Cleavers                     287\n",
       "Black-grass                  263\n",
       "Shepherds Purse              231\n",
       "Maize                        221\n",
       "Common wheat                 221\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we have only 221 images of two of the plant species when it comes to defining a training/testing data split. As we will need to ensure that a good amount of each plant species is present in the training data for a cohesive prediction model. We can ensure this is done by setting a random seed that has a good data split of the above levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can consider the quality of the provided images. After a small look through some of the photos I have noticed that the images vary in size quite a bit. To rectify this we can consider setting a base image size and transform all images to this agreed size. Firstly, we can consider graphing a histogram of all of the image heights and image widths and find the smallest height and width. To do so we can add two more columns to our pre-pixel data which will be the image height and image width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_list = [ cv2.imread(file).shape for file in full_data['file_name'] ]\n",
    "heights = [ x[0] for x in shape_list ]\n",
    "widths = [ x[1] for x in shape_list ]\n",
    "full_data['image_height'] = heights\n",
    "full_data['image_width'] = widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>numerical_species</th>\n",
       "      <th>file_name</th>\n",
       "      <th>pre_processed_file_name</th>\n",
       "      <th>image_height</th>\n",
       "      <th>image_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0050f38b3.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0050f38b3.png</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0183fdf68.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0183fdf68.png</td>\n",
       "      <td>388</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/0260cffa8.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/0260cffa8.png</td>\n",
       "      <td>886</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/05eedce4d.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/05eedce4d.png</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "      <td>./train/Black-grass/075d004bc.png</td>\n",
       "      <td>./pre_processed_train/Black-grass/075d004bc.png</td>\n",
       "      <td>471</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       species  numerical_species                          file_name  \\\n",
       "0  Black-grass                  0  ./train/Black-grass/0050f38b3.png   \n",
       "1  Black-grass                  0  ./train/Black-grass/0183fdf68.png   \n",
       "2  Black-grass                  0  ./train/Black-grass/0260cffa8.png   \n",
       "3  Black-grass                  0  ./train/Black-grass/05eedce4d.png   \n",
       "4  Black-grass                  0  ./train/Black-grass/075d004bc.png   \n",
       "\n",
       "                           pre_processed_file_name  image_height  image_width  \n",
       "0  ./pre_processed_train/Black-grass/0050f38b3.png           196          196  \n",
       "1  ./pre_processed_train/Black-grass/0183fdf68.png           388          388  \n",
       "2  ./pre_processed_train/Black-grass/0260cffa8.png           886          886  \n",
       "3  ./pre_processed_train/Black-grass/05eedce4d.png           117          117  \n",
       "4  ./pre_processed_train/Black-grass/075d004bc.png           471          471  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['image_height'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['image_width'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as shrinking an image down to a smaller size is a much better task than zooming an image in and interpolating new pixel data, we can select a base image size for our data to be 48-by-48 pixels. As this ensures that no image will be upscaled.\n",
    "\n",
    "Next, before we resize the image we can do some neat computer vision tricks. After some observation of the data it can be seen that all of these plants are distinctly green and more importantly all of these plants are photographed from the top down to their rocky garden bed. This means that in all of the given images we have a green plant photographed against a non-green background. So we can consider removing these backgrounds from all of the images and just leaving the green plant. One way we can do so is by throwing away all non-green pixels from each image by applying a mask that filters all non-green. This is done prior to resizing so that each resized image has a well defined set of green pixels and is not just plain black.\n",
    "\n",
    "Another very important issue solved by applying this green mask is that it increases the independence of each image. As we can assume that all of plant species are located in a close proximity to each other. i.e. each photographed plant species is located in the same garden bed. So by removing the background we remove the capability of our (to be constructed) neural network from learning the background of the garden bed of each plant species. This means our model will be much better in identifying the actual plant and not just the plant and environment around the plant. We also add some Gaussian blur to de-noise the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('pre_processed_train')\n",
    "for species in species_list:\n",
    "    os.mkdir('./pre_processed_train/{}'.format(species))\n",
    "save = 0\n",
    "for file in full_data['file_name']:\n",
    "    image = cv2.imread(file)\n",
    "    greenmask = cv2.inRange(cv2.cvtColor(image, cv2.COLOR_BGR2HSV), (28, 25, 25), (92, 255, 255))\n",
    "    green_image = np.zeros_like(image)\n",
    "    green_image[greenmask > 0] = image[greenmask > 0]\n",
    "    #Saving an example image for the report\n",
    "    if(save == 0):\n",
    "        cv2.imwrite('example_image.png', image)\n",
    "        cv2.imwrite('green_image.png', green_image)\n",
    "        save = 1\n",
    "    pre_processed_image = cv2.resize(green_image, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    pre_processed_image = cv2.GaussianBlur(pre_processed_image, (5,5), 0)\n",
    "    pre_processed_file = './pre_processed_train' + file[7:]\n",
    "    cv2.imwrite(pre_processed_file, pre_processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sequentially setup our convolutional neural network that be our predictive model for this multi-level classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "convolutional_neural_network = Sequential()\n",
    "# First layer\n",
    "convolutional_neural_network.add(Conv2D(32, kernel_size = (9, 9), padding = 'same', activation = 'relu', input_shape = (48, 48, 3)))\n",
    "convolutional_neural_network.add(Conv2D(64, kernel_size = (9, 9), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(MaxPooling2D(pool_size = (3, 3), strides = (2, 2)))\n",
    "convolutional_neural_network.add(BatchNormalization())\n",
    "convolutional_neural_network.add(Dropout(0.2))\n",
    "\n",
    "#Second layer\n",
    "convolutional_neural_network.add(Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(MaxPooling2D(pool_size = (3, 3), strides = (2, 2)))\n",
    "convolutional_neural_network.add(BatchNormalization())\n",
    "convolutional_neural_network.add(Dropout(0.2))\n",
    "\n",
    "# Third layer\n",
    "convolutional_neural_network.add(Conv2D(512, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(Conv2D(1024, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(MaxPooling2D(pool_size = (3, 3), strides = (2, 2)))\n",
    "convolutional_neural_network.add(BatchNormalization())\n",
    "convolutional_neural_network.add(Dropout(0.2))\n",
    "\n",
    "# Fourth layer\n",
    "convolutional_neural_network.add(Conv2D(512, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(Conv2D(1024, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "convolutional_neural_network.add(MaxPooling2D(pool_size = (3, 3), strides = (2, 2)))\n",
    "convolutional_neural_network.add(BatchNormalization())\n",
    "convolutional_neural_network.add(Dropout(0.2))\n",
    "\n",
    "# Final layer\n",
    "convolutional_neural_network.add(Flatten())\n",
    "convolutional_neural_network.add(Dense(512, activation = 'relu'))\n",
    "convolutional_neural_network.add(Dense(12, activation = 'softmax'))\n",
    "convolutional_neural_network.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can consider splitting the given data into an 80% training set and 20% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loose Silky-bent             513\n",
       "Common Chickweed             496\n",
       "Scentless Mayweed            423\n",
       "Small-flowered Cranesbill    395\n",
       "Fat Hen                      378\n",
       "Charlock                     305\n",
       "Sugar beet                   299\n",
       "Cleavers                     233\n",
       "Black-grass                  211\n",
       "Shepherds Purse              190\n",
       "Common wheat                 181\n",
       "Maize                        176\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(full_data, test_size = 0.2, random_state = 6)\n",
    "training_data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can transform the features $X$ and classifications $y$ into the format required by the neural network. So in the case of the features we just read each image from the pre-processed file locations and convert into a numpy array. As for the classifications we just need to encode them as binary dummy variables representing the different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "X_train = np.array([ cv2.imread(pre_processed_file) for pre_processed_file in training_data['pre_processed_file_name'] ])\n",
    "y_train = to_categorical(training_data['numerical_species'])\n",
    "X_test = np.array([ cv2.imread(pre_processed_file) for pre_processed_file in testing_data['pre_processed_file_name'] ])\n",
    "y_test = to_categorical(testing_data['numerical_species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can add a very useful addition to the data that we feed into the neural network. The following image data generator will alter each of the images used in each of the epochs such that the same data is not used on every single loop through the training process. Some of the alterations include: rotating the image, shifting the image to the left or right and even zooming in the image. This makes our trained neural network more robust in that it will not just be trained on the same set of data each time. So it will be able to just learn the distinguishing features between the species. This also helps a great deal in not overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "data_generator = ImageDataGenerator(rotation_range = 360, width_shift_range = 0.5, height_shift_range = 0.5,\n",
    "                                    shear_range = 0.1, zoom_range = [0.75, 1.25], horizontal_flip = True,\n",
    "                                    vertical_flip = True, fill_mode = 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally we can perform the training process and on each epoch we can also display the accuracy on the testing data. As I have access to a GPU I went a little bit overboard with the number of training loops as well as the number of images process at a time. My PC was able to handle a batch size of 32 and for each of the below main epochs I was able to complete 20 actual epochs. So in total the output from the following represents 600 epochs or training loops through the image data generated training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2375/2375 [==============================] - 245s 103ms/step - loss: 1.4389 - acc: 0.5072 - val_loss: 1.6053 - val_acc: 0.5684\n",
      "Epoch 2/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.8803 - acc: 0.6980 - val_loss: 0.4862 - val_acc: 0.8116\n",
      "Epoch 3/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.6637 - acc: 0.7726 - val_loss: 0.3703 - val_acc: 0.8705\n",
      "Epoch 4/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.5524 - acc: 0.8095 - val_loss: 0.2819 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.4811 - acc: 0.8324 - val_loss: 0.2524 - val_acc: 0.9011\n",
      "Epoch 6/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.4303 - acc: 0.8499 - val_loss: 0.2513 - val_acc: 0.9063\n",
      "Epoch 7/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.3972 - acc: 0.8608 - val_loss: 0.2887 - val_acc: 0.9095\n",
      "Epoch 8/30\n",
      "2375/2375 [==============================] - 240s 101ms/step - loss: 0.3727 - acc: 0.8691 - val_loss: 0.2338 - val_acc: 0.9211\n",
      "Epoch 9/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.3504 - acc: 0.8775 - val_loss: 0.2374 - val_acc: 0.9232\n",
      "Epoch 10/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.3378 - acc: 0.8829 - val_loss: 0.2264 - val_acc: 0.9253\n",
      "Epoch 11/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.3082 - acc: 0.8910 - val_loss: 0.3572 - val_acc: 0.8853\n",
      "Epoch 12/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.3055 - acc: 0.8930 - val_loss: 0.1955 - val_acc: 0.9253\n",
      "Epoch 13/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.2781 - acc: 0.9008 - val_loss: 0.1835 - val_acc: 0.9411\n",
      "Epoch 14/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.2664 - acc: 0.9054 - val_loss: 0.2818 - val_acc: 0.9021\n",
      "Epoch 15/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.2563 - acc: 0.9088 - val_loss: 0.1791 - val_acc: 0.9400\n",
      "Epoch 16/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.2494 - acc: 0.9117 - val_loss: 0.1695 - val_acc: 0.9389\n",
      "Epoch 17/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.2341 - acc: 0.9163 - val_loss: 0.1314 - val_acc: 0.9558\n",
      "Epoch 18/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.2358 - acc: 0.9165 - val_loss: 0.2048 - val_acc: 0.9411\n",
      "Epoch 19/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.2143 - acc: 0.9232 - val_loss: 0.2032 - val_acc: 0.9347\n",
      "Epoch 20/30\n",
      "2375/2375 [==============================] - 241s 101ms/step - loss: 0.2110 - acc: 0.9243 - val_loss: 0.2106 - val_acc: 0.9389\n",
      "Epoch 21/30\n",
      "2375/2375 [==============================] - 251s 106ms/step - loss: 0.1998 - acc: 0.9294 - val_loss: 0.1436 - val_acc: 0.9537\n",
      "Epoch 22/30\n",
      "2375/2375 [==============================] - 252s 106ms/step - loss: 0.2018 - acc: 0.9283 - val_loss: 0.1655 - val_acc: 0.9463\n",
      "Epoch 23/30\n",
      "2375/2375 [==============================] - 249s 105ms/step - loss: 0.1998 - acc: 0.9290 - val_loss: 0.1552 - val_acc: 0.9537\n",
      "Epoch 24/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.1855 - acc: 0.9345 - val_loss: 0.2382 - val_acc: 0.9189\n",
      "Epoch 25/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.1878 - acc: 0.9330 - val_loss: 0.1795 - val_acc: 0.9389\n",
      "Epoch 26/30\n",
      "2375/2375 [==============================] - 241s 102ms/step - loss: 0.1805 - acc: 0.9374 - val_loss: 0.2544 - val_acc: 0.9242\n",
      "Epoch 27/30\n",
      "2375/2375 [==============================] - 242s 102ms/step - loss: 0.1709 - acc: 0.9395 - val_loss: 0.2285 - val_acc: 0.9463\n",
      "Epoch 28/30\n",
      "2375/2375 [==============================] - 242s 102ms/step - loss: 0.1769 - acc: 0.9383 - val_loss: 0.1728 - val_acc: 0.9537\n",
      "Epoch 29/30\n",
      "2375/2375 [==============================] - 242s 102ms/step - loss: 0.1608 - acc: 0.9437 - val_loss: 0.1399 - val_acc: 0.9589\n",
      "Epoch 30/30\n",
      "2375/2375 [==============================] - 242s 102ms/step - loss: 0.1700 - acc: 0.9416 - val_loss: 0.1573 - val_acc: 0.9558\n"
     ]
    }
   ],
   "source": [
    "convolutional_neural_network.fit_generator(data_generator.flow(X_train, y_train, batch_size = 32), validation_data = (X_test, y_test),\n",
    "                                          steps_per_epoch = 20*len(X_train)/32, epochs = 30);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
