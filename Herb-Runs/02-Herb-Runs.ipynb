{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling herb runs in Old School Runescape\n",
    "\n",
    "## 02-Theoretical framework for estimation \n",
    "If we define the extra amount of herbs harvested on a single herb patch as a random variable $X$ say, then we can consider modelling it as a negative binomial random variable. As a player continues harvesting extra herbs until the first failure. Each individual harvest of a herb can be treated as a Bernoulli trial with some unknown fixed probability of success $p$ say. This probability may have different dependenies such as: what type of herb is planted, what the players farming level is, and which of the 9 herb patches the herbs are being harvested from. Hence, this problem requires some sort of method to solve for this unknown probability $p$ on each herb patch. Once found we can construct the particular negative binomial distribution in question and do some exploratory simulation.\n",
    "\n",
    "We can use the collected data to obtain a maximum likelihood estimate (MLE) of $p$. This type of statistical inference is known as Maximum Liklihood Estimation and the end result is a very good unbiased estimate based on the collected data which has many useful properties that we want in an estimator (read more on MLEs in this great [article](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)). To do this maximum likelihood estimation we need to contruct a likelihood function from the known probability mass function of a negative binomial random variable. In general if we let $Y\\sim {NB}(1,p)$ then as listed [here](https://en.wikipedia.org/wiki/Negative_binomial_distribution), the probability mass function (PMF) is\n",
    "$$P(Y=y) = \\binom{y+1-1}{y} p^{y}(1-p)^{1} = p^y(1-p).$$\n",
    "\n",
    "Now we need to consider switching to a likelihood based perspective. If we let $\\theta$ be a guess of $p$ based on the collected data labelled $Y_1=y_1, Y_2=y_2,\\dots Y_n=y_n$ for $n$ observations of extra herbs harvested at one particular patch. Then the probability that the chosen $\\theta$ fits an individual observation is the same as above now with fixed $y_i$ (for $i$ labelling each data-point) in place of fixed $p$\n",
    "$$l(\\theta\\ \\vert \\ Y_i=y_i) = P_{\\theta} (Y_i=y_i) = \\theta^{y_i}(1-\\theta).$$\n",
    "\n",
    "Which gives the joint probability that the chosen $\\theta$ fits all of the data as the product of all of these individual PMFs (as each number of extra herbs harvested at each patch is independent and identically distributed)\n",
    "$$L\\left(\\theta \\ \\vert \\ Y_1=y_1, Y_2=y_2,\\dots Y_n=y_n\\right) = \\prod_{i=1}^n \\theta^{y_i}(1-\\theta).$$\n",
    "\n",
    "To find the MLE based on our given data we need to maximise the above function in terms of choice of $\\theta$. Maximising such a function is quite difficult so we can consider using a log transform to transform the above from a product to a summation. This will have no effect on the $\\theta$ that maximises $L$. We compute the log likelihood by\n",
    "$$\\begin{aligned} \n",
    "\\mathcal{L}\\left(\\theta \\ \\vert \\ Y_1=y_1, Y_2=y_2,\\dots Y_n=y_n\\right) &= \\log\\{L\\left(\\theta \\ \\vert \\ Y_1=y_1, Y_2=y_2,\\dots Y_n=y_n\\right)\\} \\\\\n",
    "&= \\log\\left\\{\\prod_{i=1}^n \\theta^{y_i}(1-\\theta)\\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\log\\left\\{\\theta^{y_i}(1-\\theta)\\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\left[y_i\\log(\\theta) + \\log(1-\\theta)\\right] \\\\\n",
    "&= \\log(\\theta)\\sum_{i=1}^n y_i + n\\log(1-\\theta).\n",
    "\\end{aligned}$$\n",
    "\n",
    "We then need to consider maximising this log-likelihood to find the MLE. To find extrema of the above function we take the first derivative (called the score) and set it to zero such that\n",
    "$$\\begin{aligned}\n",
    "\\frac{dL}{d\\theta} &= 0 \\\\\n",
    "\\frac{d}{d\\theta}\\left[\\log(\\theta)\\sum_{i=1}^n y_i + n\\log(1-\\theta)\\right] &= 0 \\\\\n",
    "\\frac{1}{\\theta}\\sum_{i=1}^n y_i - \\frac{n}{1-\\theta} &= 0 \\\\\n",
    "\\frac{1}{\\theta}\\sum_{i=1}^n y_i &= \\frac{n}{1-\\theta} \\\\\n",
    "(1-\\theta)\\sum_{i=1}^n y_i &= n\\theta \\\\\n",
    "\\sum_{i=1}^n y_i - \\theta \\sum_{i=1}^n y_i &= n\\theta \\\\\n",
    "\\sum_{i=1}^n y_i &= n\\theta + \\theta \\sum_{i=1}^n y_i \\\\\n",
    "\\theta \\left(n + \\sum_{i=1}^n y_i\\right) &= \\sum_{i=1}^n y_i \\\\\n",
    "\\theta &= \\frac{\\sum_{i=1}^n y_i}{n + \\sum_{i=1}^n y_i}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now for the sake of clarity consider labelling this extrema as $\\vartheta$, we can divide both numerator and denominator of the resulting fraction by $n$ to find a nice expression just in terms of the mean of the data $\\bar{Y}$\n",
    "$$\\vartheta = \\frac{\\bar{Y}}{1+\\bar{Y}}.$$\n",
    "\n",
    "Next we need to check if this extrema is a maximum or a minimum of the score by the second derivative test. If this extrema is a maximum it will be the MLE\n",
    "$$\\begin{aligned}\n",
    "\\frac{d^2L}{d\\theta^2} &= \\frac{d}{d\\theta}\\left[\\frac{dL}{d\\theta}\\right] \\\\\n",
    "&= \\frac{d}{d\\theta}\\left[\\frac{1}{\\theta}\\sum_{i=1}^n y_i - \\frac{n}{1-\\theta}\\right] \\\\\n",
    "&= -\\frac{1}{\\theta^2}\\sum_{i=1}^n y_i - \\frac{n}{(1-\\theta)^2}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can see that for any $\\vartheta$ in its specified range $[0,1]$, that the second derivative will be negative. So by the second derivative test we find this extrema to be a maximum. Hence, the MLE of $p$\n",
    "$$\\text{MLE}(p) = \\frac{\\bar{Y}}{1+\\bar{Y}}.$$\n",
    "\n",
    "We can use this MLE as our estimate of the true probability of successfully picking an extra herb at a particular patch based on the collected data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
